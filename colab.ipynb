{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4okHE4FoXfiQ"
   },
   "source": [
    "# Applications of Reinforcement Learning in Finance - Deep Q-learning & the S&P500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5SekTJXXfiT"
   },
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKFgy8b6XfiU"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22145,
     "status": "ok",
     "timestamp": 1654303210525,
     "user": {
      "displayName": "Frensi Zejnullahu",
      "userId": "02346894623647773197"
     },
     "user_tz": -120
    },
    "id": "1hhpkLzFXfiV",
    "outputId": "d82a3741-5118-438a-cfd4-c4c620b91cf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n",
      "Your runtime has 8.6 gigabytes of available RAM\n",
      "\n",
      "Not using a high-RAM runtime\n",
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive          # Use on Google Colab\n",
    "#drive.mount('/content/gdrive')          # Otherwise Comment out\n",
    "#%cd /content/gdrive/MyDrive/\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "results_path = Path('result')         # Results directory\n",
    "if not results_path.exists():           # Set up if not existing\n",
    "  results_path.mkdir(parents=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from time import time\n",
    "from collections import deque\n",
    "from random import sample\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)\n",
    "  from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')\n",
    "\n",
    "  # Use GPU if available\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "else:\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4soQqr898RiW"
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 788
    },
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1654278481162,
     "user": {
      "displayName": "Frensi Zejnullahu",
      "userId": "02346894623647773197"
     },
     "user_tz": -120
    },
    "id": "t8r9-TvR8WI2",
    "outputId": "5f03c98e-8563-430f-fd12-a09907a724cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'Trading costs: 0.10% | Time costs: 0.010%'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = 3                           # Model for Testing\n",
    "\n",
    "\n",
    "total_steps = 0\n",
    "max_episodes = 20                   # Episodes to train (change to 1000)\n",
    "\n",
    "gamma = .90,                        # discount factor\n",
    "tau = 100                           # target network update frequency\n",
    "\n",
    "## NN Architecture\n",
    "architecture = (64, 64)             # units per layer (maybe 256)\n",
    "learning_rate = 0.0001              # learning rate\n",
    "l2_reg = 1e-6                       # L2 regularization\n",
    "\n",
    "## Experience Replay\n",
    "replay_capacity = int(1e6)          # Capacity\n",
    "batch_size = 4096                   # for training set to 4096, the batch size to train the NNs on\n",
    "\n",
    "## epsilon-greedy Policy\n",
    "epsilon_start = 1.0                 # epsilon start value\n",
    "epsilon_end = .01                   # where it stops\n",
    "epsilon_decay_steps = 250           # steps to decay\n",
    "epsilon_exponential_decay = .99     # exponential decay multiplicator\n",
    "\n",
    "trading_cost_bps = 1e-3\n",
    "time_cost_bps = 1e-4\n",
    "f'Trading costs: {trading_cost_bps:.2%} | Time costs: {time_cost_bps:.3%}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RP3fITvTXfiY"
   },
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3627,
     "status": "ok",
     "timestamp": 1654278486908,
     "user": {
      "displayName": "Frensi Zejnullahu",
      "userId": "02346894623647773197"
     },
     "user_tz": -120
    },
    "id": "9ussIycaXfiY",
    "outputId": "d58793c3-da3e-4370-96ea-18e4430698cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:trading_env:trading_env logger started.\n",
      "INFO:trading_env:loading data for SPX...\n",
      "INFO:trading_env:got data for SPX...\n",
      "INFO:trading_env:loading data for RUS...\n",
      "INFO:trading_env:got data for RUS...\n",
      "INFO:trading_env:loading data for WTI...\n",
      "INFO:trading_env:got data for WTI...\n",
      "INFO:trading_env:loading data for GOLD...\n",
      "INFO:trading_env:got data for GOLD...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3, Episodes :20\n"
     ]
    }
   ],
   "source": [
    "## Helper Function+\n",
    "np.random.seed(10)                              # Set Random\n",
    "tf.random.set_seed(10)                          # Set Random\n",
    "\n",
    "sns.set_style('whitegrid')                      # Plot Style\n",
    "\n",
    "def format_time(t):                             # Helper Function\n",
    "    m_, s = divmod(t, 60)\n",
    "    h, m = divmod(m_, 60)\n",
    "    return '{:02.0f}:{:02.0f}:{:02.0f}'.format(h, m, s)\n",
    "\n",
    "## Set up Gym Environment\n",
    "trading_days = 252\n",
    "register(\n",
    "    id='trading-v0',\n",
    "    entry_point = \"trading_env:TradingEnvironment\",\n",
    "    max_episode_steps=trading_days)\n",
    "\n",
    "\n",
    "\n",
    "## Initialize Trading Environment\n",
    "\n",
    "trading_environment = gym.make('trading-v0',\n",
    "                               ticker='SPX',\n",
    "                               trading_days=trading_days,\n",
    "                               trading_cost_bps=trading_cost_bps,\n",
    "                               time_cost_bps=time_cost_bps,\n",
    "                               model = model)\n",
    "trading_environment.seed(10)\n",
    "\n",
    "state_dim = trading_environment.observation_space                   # Input Nodes\n",
    "num_actions = trading_environment.action_space.n                    # Number of Actions\n",
    "max_episode_steps = trading_environment.spec.max_episode_steps      # Episode Length (252)\n",
    "\n",
    "if state_dim == 2:\n",
    "  print(\"Model 0,\", \"Episodes :\" + str(max_episodes))\n",
    "elif state_dim == 4:\n",
    "  print(\"Model 1,\", \"Episodes :\" + str(max_episodes))\n",
    "elif state_dim == 6:\n",
    "  print(\"Model 2,\", \"Episodes :\" + str(max_episodes))\n",
    "elif state_dim == 8:\n",
    "  print(\"Model 3,\", \"Episodes :\" + str(max_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SpuP24pXfii"
   },
   "source": [
    "## Trading Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsjUsKBA-0u1"
   },
   "source": [
    "### Define Trading Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QENRfl4hXfij"
   },
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    def __init__(self, state_dim,\n",
    "                 num_actions,\n",
    "                 learning_rate,\n",
    "                 gamma,\n",
    "                 epsilon_start,\n",
    "                 epsilon_end,\n",
    "                 epsilon_decay_steps,\n",
    "                 epsilon_exponential_decay,\n",
    "                 replay_capacity,\n",
    "                 architecture,\n",
    "                 l2_reg,\n",
    "                 tau,\n",
    "                 batch_size):\n",
    "\n",
    "        self.state_dim = state_dim                              # Input Nodes\n",
    "        self.num_actions = num_actions                          # Number of Actions\n",
    "        self.experience = deque([], maxlen=replay_capacity)     # Where to store trained data\n",
    "        self.learning_rate = learning_rate                      # DDQN learning rate\n",
    "        self.gamma = gamma                                      # Discountfactor for future Rewards\n",
    "        self.architecture = architecture                        # Architecture of the DDQN\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "        self.online_network = self.build_model()                # Trainable Network\n",
    "        self.target_network = self.build_model(trainable=False) # Target Network (Second Network)\n",
    "        self.update_target()                                    # Update Target Network with weight of Online Network\n",
    "\n",
    "        self.epsilon = epsilon_start                            # Epsilon = 1\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps          # Decay steps of Epsilon\n",
    "        self.epsilon_decay = (epsilon_start - epsilon_end) / epsilon_decay_steps    # How much to decay.\n",
    "        self.epsilon_exponential_decay = epsilon_exponential_decay\n",
    "        self.epsilon_history = []                               # Store epsilon values\n",
    "\n",
    "        self.total_steps = self.train_steps = 0                         # Number of steps taken\n",
    "        self.episodes = self.episode_length = self.train_episodes = 0   # Number of episodes\n",
    "        self.steps_per_episode = []\n",
    "        self.episode_reward = 0\n",
    "        self.rewards_history = []\n",
    "\n",
    "        self.batch_size = batch_size        # Number of batches for DDQN to train\n",
    "        self.tau = tau                      # Steps until the update of Target Network\n",
    "        self.losses = []\n",
    "        self.idx = tf.range(batch_size)\n",
    "        self.train = True\n",
    "\n",
    "    # Creation of Online Network with Keras (Architecture, etc.)\n",
    "    def build_model(self, trainable=True):\n",
    "        layers = []\n",
    "        n = len(self.architecture)\n",
    "        for i, units in enumerate(self.architecture, 1):\n",
    "            # Dense Layer No.1\n",
    "            layers.append(Dense(units=units,\n",
    "                                # state_dim as input nodes\n",
    "                                input_dim=self.state_dim if i == 1 else None,\n",
    "                                activation='relu',  # Rectified Linear Unit\n",
    "                                kernel_regularizer=l2(self.l2_reg),\n",
    "                                name=f'Dense_{i}',\n",
    "                                trainable=trainable))\n",
    "            # Dropout Layer\n",
    "        layers.append(Dropout(.1))\n",
    "            # Dense Layer No.2\n",
    "        layers.append(Dense(units=self.num_actions,\n",
    "                            trainable=trainable,\n",
    "                            name='Output'))\n",
    "\n",
    "        model = Sequential(layers)                              # Initialize the model the model\n",
    "        model.compile(loss='mean_squared_error',                # Compile the model\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # Update Target Network with weights of Online Network\n",
    "    def update_target(self):\n",
    "        self.target_network.set_weights(self.online_network.get_weights())\n",
    "\n",
    "    # Epsilon-Greedy Policy: Either Random action or based on Online Network prediciton.\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        self.total_steps += 1\n",
    "        # Random Number below Epsilon take random action\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        # Take action based on Online Network prediciton with highest Q-value\n",
    "        q = self.online_network.predict(state)\n",
    "        return np.argmax(q, axis=1).squeeze()\n",
    "\n",
    "    # Stores state, action, reward, next_state\n",
    "    def memorize_transition(self, s, a, r, s_prime, not_done):\n",
    "        # Check if 252 day are over\n",
    "        if not_done:\n",
    "            self.episode_reward += r\n",
    "            self.episode_length += 1\n",
    "        # If 1 Episode is over.\n",
    "        else:\n",
    "            if self.train:\n",
    "                # Decrease epsilon incrementally or exponentially.\n",
    "                if self.episodes < self.epsilon_decay_steps:\n",
    "                    self.epsilon -= self.epsilon_decay\n",
    "                else:\n",
    "                    self.epsilon *= self.epsilon_exponential_decay\n",
    "\n",
    "            # Append new data/numbers\n",
    "            self.episodes += 1\n",
    "            self.rewards_history.append(self.episode_reward)\n",
    "            self.steps_per_episode.append(self.episode_length)\n",
    "            self.episode_reward, self.episode_length = 0, 0\n",
    "        # Store as experience\n",
    "        self.experience.append((s, a, r, s_prime, not_done))\n",
    "\n",
    "    # Experience Replay method for training the DDQN\n",
    "    def experience_replay(self):\n",
    "        # Start Experience Replay when experience is longer than batch_size (4096)\n",
    "        if self.batch_size > len(self.experience):\n",
    "            return\n",
    "        # Minibatch of data\n",
    "        minibatch = map(np.array, zip(*sample(self.experience, self.batch_size)))\n",
    "        states, actions, rewards, next_states, not_done = minibatch\n",
    "        # Predicted Q-Value for next state and take the best one.\n",
    "        next_q_values = self.online_network.predict_on_batch(next_states)\n",
    "        best_actions = tf.argmax(next_q_values, axis=1)\n",
    "\n",
    "        # Predicted Q-Value from Target Network.\n",
    "        next_q_values_target = self.target_network.predict_on_batch(next_states)\n",
    "        target_q_values = tf.gather_nd(next_q_values_target,\n",
    "                                       tf.stack((self.idx, tf.cast(best_actions, tf.int32)), axis=1))\n",
    "\n",
    "        # Expected Q-Value the target with reward and discount factor.\n",
    "        targets = rewards + not_done * self.gamma * target_q_values\n",
    "\n",
    "        # Predicted Q-Value and store the new target Q-Value\n",
    "        q_values = self.online_network.predict_on_batch(states)\n",
    "        q_values[[self.idx, actions]] = targets\n",
    "\n",
    "        # Train network with (s', a') and store loss\n",
    "        loss = self.online_network.train_on_batch(x=states, y=q_values)\n",
    "        self.losses.append(loss)\n",
    "\n",
    "        # If tau steps are reached update the target\n",
    "        if self.total_steps % self.tau == 0:\n",
    "            self.update_target()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9PBbINVXfim"
   },
   "source": [
    "### Create DDQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4273,
     "status": "ok",
     "timestamp": 1654278495891,
     "user": {
      "displayName": "Frensi Zejnullahu",
      "userId": "02346894623647773197"
     },
     "user_tz": -120
    },
    "id": "XPlx92nrXfio",
    "outputId": "c32210fe-d1e1-4db4-8031-04b582f2b2f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Dense_1 (Dense)             (None, 64)                576       \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " Output (Dense)              (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,931\n",
      "Trainable params: 4,931\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 23:46:21.015455: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "ddqn = DDQNAgent(state_dim=state_dim,\n",
    "                 num_actions=num_actions,\n",
    "                 learning_rate=learning_rate,\n",
    "                 gamma=gamma,\n",
    "                 epsilon_start=epsilon_start,\n",
    "                 epsilon_end=epsilon_end,\n",
    "                 epsilon_decay_steps=epsilon_decay_steps,\n",
    "                 epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "                 replay_capacity=replay_capacity,\n",
    "                 architecture=architecture,\n",
    "                 l2_reg=l2_reg,\n",
    "                 tau=tau,\n",
    "                 batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "## Store Result Function\n",
    "\n",
    "def save_training(results_path, max_episodes, navs, market_navs, diffs):\n",
    "        training_results = pd.DataFrame({'Episode': list(range(1, len(navs)+1)),\n",
    "                                         'Agent Ep NAV': navs,\n",
    "                                         'Market Ep NAV': market_navs,\n",
    "                                         'Agent CumSum NAV': np.cumsum(navs),\n",
    "                                         'Market CumSum NAV': np.cumsum(market_navs),\n",
    "                                         'Difference': diffs}).set_index('Episode')\n",
    "        training_results['Strategy Wins (%)'] = (training_results.Difference > 0).rolling(100).sum() # Win in % of last 100 episodes\n",
    "        training_results.to_csv(results_path / 'training_results.csv', index=True)\n",
    "        print(training_results.info())\n",
    "\n",
    "\n",
    "def save_test(results_path, test_date, test_action, ret_agent, ret_market):\n",
    "    test_results = pd.DataFrame({'Date': test_date,\n",
    "                                 'Action': test_action,\n",
    "                                 'Agent Return': ret_agent,\n",
    "                                 'Market Return': ret_market,\n",
    "                                 \"Agent NAV\": np.cumsum(ret_agent),\n",
    "                                 \"Market NAV\": np.cumsum(ret_market)}).set_index('Date')\n",
    "    test_results.to_csv(results_path / 'test_results.csv', index=True)\n",
    "    print(test_results.info())\n",
    "\n",
    "## Test model Function\n",
    "\n",
    "def testing_model(ddqn, test_len):\n",
    "    ddqn_test = ddqn\n",
    "\n",
    "    test_date = []\n",
    "    test_action = []\n",
    "    ret_agent = []\n",
    "    ret_market = []\n",
    "    for episode in range(1, test_len+1):\n",
    "        this_state = trading_environment.reset(training=False)            # S_t = [R1, R5] (Reset the Environment) training = False\n",
    "\n",
    "        for episode_step in range(max_episode_steps):       # [0,....,251]\n",
    "            action = ddqn_test.epsilon_greedy_policy(this_state.reshape(-1, state_dim))  # A_t = [0, 1, 2] (Gives Action)\n",
    "\n",
    "            # Execute Action\n",
    "            next_state, reward, done, _ = trading_environment.step(action)      # S_t+1, R_t, done\n",
    "\n",
    "            # Performance Data\n",
    "            test_action.append(int(action))\n",
    "            test_date.append(trading_environment.data_source.date)\n",
    "            ret_agent.append(reward)\n",
    "            ret_market.append(trading_environment.data_source.target)\n",
    "\n",
    "            # Store Results\n",
    "            ddqn_test.memorize_transition(this_state,\n",
    "                                          action,\n",
    "                                          reward,\n",
    "                                          next_state,\n",
    "                                          0.0 if done else 1.0) # If episode is over.\n",
    "            # Train Network\n",
    "            if ddqn_test.train:\n",
    "                ddqn_test.experience_replay()\n",
    "            if done:\n",
    "                break\n",
    "            # New state for next sequence\n",
    "            this_state = next_state\n",
    "    trading_environment.close()\n",
    "    return test_date, test_action, ret_agent, ret_market\n",
    "\n",
    "## Visualization\n",
    "\n",
    "def track_results(episode, total,\n",
    "                  nav_ma_100, nav_ma_10,\n",
    "                  market_nav_100, market_nav_10,\n",
    "                  win_ratio, epsilon):\n",
    "\n",
    "    time_ma = np.mean([episode_time[-100:]])                            # Currently no meaning\n",
    "    T = np.sum(episode_time)                                            # Currently no meaning\n",
    "\n",
    "    ## Outputs while training\n",
    "    pretext = \"Training Results\"\n",
    "    template = '{:>4d} | {} | Agent: {:>6.1%} ({:>6.1%}) | '\n",
    "    template += 'Market: {:>6.1%} ({:>6.1%}) | '\n",
    "    template += 'Wins: {:>5.1%} | eps: {:>6.3f}'\n",
    "    # Number of episode, time, Moving Average of last 100 / 10 obs, win_ratio of trading agent, epsilon value.\n",
    "    print(pretext + template.format(episode, format_time(total),\n",
    "                                    nav_ma_100, nav_ma_10,\n",
    "                                    market_nav_100, market_nav_10,\n",
    "                                    win_ratio, epsilon))\n",
    "\n",
    "ddqn.online_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-OJMxB6Xfip"
   },
   "source": [
    "## Train and Test Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "G-jwHohcXfiu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results  20 | 00:00:60 | Agent: -22.5% (-16.2%) | Market:   8.1% (  8.0%) | Wins: 10.0% | eps:  0.921\n",
      "Final Stats:\n",
      "Training Results  20 | 00:00:60 | Agent: -25.0% (-22.5%) | Market:   4.7% (  8.1%) | Wins: 15.0% | eps:  0.921\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20 entries, 1 to 20\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Agent Ep NAV       20 non-null     float64\n",
      " 1   Market Ep NAV      20 non-null     float64\n",
      " 2   Agent CumSum NAV   20 non-null     float64\n",
      " 3   Market CumSum NAV  20 non-null     float64\n",
      " 4   Difference         20 non-null     float64\n",
      " 5   Strategy Wins (%)  0 non-null      float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 1.1 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 252 entries, 2021-05-20 to 2022-05-18\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Action         252 non-null    int64  \n",
      " 1   Agent Return   252 non-null    float64\n",
      " 2   Market Return  252 non-null    float64\n",
      " 3   Agent NAV      252 non-null    float64\n",
      " 4   Market NAV     252 non-null    float64\n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 11.8 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "results = []\n",
    "## Initialize Variables\n",
    "episode_time, navs, market_navs, diffs = [], [], [], []\n",
    "print(\"-----------------------------------------------\")\n",
    "print(\"model: \", model)\n",
    "print(\"trading_cost_bps: \", trading_cost_bps)\n",
    "print(\"time_cost_bps: \", time_cost_bps)\n",
    "print(\"batch_size: \", batch_size)\n",
    "print(\"max_episodes: \", max_episodes)\n",
    "print(\"epsilon_decay_steps: \", epsilon_decay_steps)\n",
    "print(\"-----------------------------------------------\")\n",
    "for episode in range(1, max_episodes + 1):\n",
    "\n",
    "    this_state = trading_environment.reset()            # S_t = [R1, R5] (Reset the Environment)\n",
    "    for episode_step in range(max_episode_steps):       # [0,....,251]\n",
    "        # A_t = [0, 1, 2] (Gives Action)\n",
    "        action = ddqn.epsilon_greedy_policy(this_state.reshape(-1, state_dim))\n",
    "        # Execute Action\n",
    "        next_state, reward, done, _ = trading_environment.step(action)      # S_t+1, R_t, done\n",
    "\n",
    "        # Store Results\n",
    "        ddqn.memorize_transition(this_state,\n",
    "                                 action,\n",
    "                                 reward,\n",
    "                                 next_state,\n",
    "                                 0.0 if done else 1.0) # If episode is over.\n",
    "        # Train Network\n",
    "        if ddqn.train:\n",
    "            ddqn.experience_replay()\n",
    "        if done:\n",
    "            break\n",
    "        # New state for next sequence\n",
    "        this_state = next_state\n",
    "\n",
    "    # get DataFrame with sequence of actions, returns and nav values\n",
    "    result = trading_environment.env.simulator.result()\n",
    "\n",
    "    # get results of last step\n",
    "    final = result.iloc[-1]\n",
    "\n",
    "    # NAV of last step\n",
    "    nav = final.nav\n",
    "    navs.append(nav)\n",
    "    # market nav\n",
    "    market_nav = final.market_nav\n",
    "    market_navs.append(market_nav)\n",
    "\n",
    "    # track difference between agent and market NAV results\n",
    "    diff = nav - market_nav\n",
    "    diffs.append(diff)\n",
    "\n",
    "    # For MA(100) --> need more than 100 episodes\n",
    "    if episode % 10 == 0:\n",
    "        track_results(episode,\n",
    "                      time() - start,\n",
    "                      # show mov. average results for 100 (10) episodes\n",
    "                      np.mean(navs[-10:]),                 # Last 100 episodes\n",
    "                      np.mean(navs[-5:]),                  # Last 10 episodes\n",
    "                      np.mean(market_navs[-10:]),          # Last 100 episodes\n",
    "                      np.mean(market_navs[-5:]),           # Last 10 episodes\n",
    "\n",
    "                      # share of agent wins, defined as higher ending nav\n",
    "                      # sum of all diffs higher than 0 (trading agent better than marktet)\n",
    "                      # from last 100 (or less) observation\n",
    "                      # divided by 100 or less (len of diffs)\n",
    "                      np.sum([s > 0 for s in diffs[-10:]])/min(len(diffs), 10),\n",
    "                      ddqn.epsilon) # epsilon value\n",
    "\n",
    "    if len(diffs) > 25 and all([r > 0 for r in diffs[-25:]]):\n",
    "        print(result.tail())\n",
    "        break\n",
    "\n",
    "print(\"Final Stats:\")\n",
    "# Same as described above\n",
    "track_results(episode,\n",
    "                      time() - start,\n",
    "                      np.mean(navs[-100:]),                 # Last 100 episodes\n",
    "                      np.mean(navs[-10:]),                  # Last 10 episodes\n",
    "                      np.mean(market_navs[-100:]),          # Last 100 episodes\n",
    "                      np.mean(market_navs[-10:]),           # Last 10 episodes\n",
    "                      np.sum([s > 0 for s in diffs[-100:]])/min(len(diffs), 100),\n",
    "                      ddqn.epsilon) # epsilon value\n",
    "\n",
    "#### Test the trained model\n",
    "\n",
    "if trading_environment.data_source.counter != 0:        # Check the test counter\n",
    "    trading_environment.data_source.counter = 0\n",
    "\n",
    "test_len = int(np.floor(len(trading_environment.data_source.y_test.index)/trading_days))    # How many years of terst data we got.\n",
    "\n",
    "if test_len < 1:\n",
    "    print(\"Your Test Data is too short.\")           # If less than one year will raise error.\n",
    "\n",
    "test_date, test_action, ret_agent, ret_market = testing_model(ddqn=ddqn, test_len=test_len) # Testing model\n",
    "\n",
    "trading_environment.close()\n",
    "\n",
    "save_training(results_path=results_path, max_episodes=max_episodes,\n",
    "              navs=navs, market_navs=market_navs, diffs=diffs)\n",
    "save_test(results_path=results_path,\n",
    "          test_date=test_date, test_action=test_action, ret_agent=ret_agent, ret_market=ret_market)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BT1w1lF3Xfiv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            Action  Agent Return  Market Return  Agent NAV  Market NAV\nDate                                                                  \n2021-05-20       1     -0.002889      -0.002789  -0.002889   -0.002789\n2021-05-21      -1     -0.012398       0.010398  -0.015287    0.007608\n2021-05-24      -1      0.000502      -0.000602  -0.014785    0.007007\n2021-05-25      -1     -0.010216       0.010116  -0.025001    0.017123\n2021-05-26      -1      0.001867      -0.001967  -0.023134    0.015156\n...            ...           ...            ...        ...         ...\n2022-05-12      -1      0.016539      -0.016639  -0.439620   -0.033575\n2022-05-13      -1      0.000663      -0.000763  -0.438957   -0.034338\n2022-05-16       0     -0.001000       0.023553  -0.439957   -0.010785\n2022-05-17       0     -0.000100      -0.003732  -0.440057   -0.014516\n2022-05-18       1      0.018976       0.019976  -0.421081    0.005460\n\n[252 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Action</th>\n      <th>Agent Return</th>\n      <th>Market Return</th>\n      <th>Agent NAV</th>\n      <th>Market NAV</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2021-05-20</th>\n      <td>1</td>\n      <td>-0.002889</td>\n      <td>-0.002789</td>\n      <td>-0.002889</td>\n      <td>-0.002789</td>\n    </tr>\n    <tr>\n      <th>2021-05-21</th>\n      <td>-1</td>\n      <td>-0.012398</td>\n      <td>0.010398</td>\n      <td>-0.015287</td>\n      <td>0.007608</td>\n    </tr>\n    <tr>\n      <th>2021-05-24</th>\n      <td>-1</td>\n      <td>0.000502</td>\n      <td>-0.000602</td>\n      <td>-0.014785</td>\n      <td>0.007007</td>\n    </tr>\n    <tr>\n      <th>2021-05-25</th>\n      <td>-1</td>\n      <td>-0.010216</td>\n      <td>0.010116</td>\n      <td>-0.025001</td>\n      <td>0.017123</td>\n    </tr>\n    <tr>\n      <th>2021-05-26</th>\n      <td>-1</td>\n      <td>0.001867</td>\n      <td>-0.001967</td>\n      <td>-0.023134</td>\n      <td>0.015156</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2022-05-12</th>\n      <td>-1</td>\n      <td>0.016539</td>\n      <td>-0.016639</td>\n      <td>-0.439620</td>\n      <td>-0.033575</td>\n    </tr>\n    <tr>\n      <th>2022-05-13</th>\n      <td>-1</td>\n      <td>0.000663</td>\n      <td>-0.000763</td>\n      <td>-0.438957</td>\n      <td>-0.034338</td>\n    </tr>\n    <tr>\n      <th>2022-05-16</th>\n      <td>0</td>\n      <td>-0.001000</td>\n      <td>0.023553</td>\n      <td>-0.439957</td>\n      <td>-0.010785</td>\n    </tr>\n    <tr>\n      <th>2022-05-17</th>\n      <td>0</td>\n      <td>-0.000100</td>\n      <td>-0.003732</td>\n      <td>-0.440057</td>\n      <td>-0.014516</td>\n    </tr>\n    <tr>\n      <th>2022-05-18</th>\n      <td>1</td>\n      <td>0.018976</td>\n      <td>0.019976</td>\n      <td>-0.421081</td>\n      <td>0.005460</td>\n    </tr>\n  </tbody>\n</table>\n<p>252 rows Ã— 5 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_res = pd.read_csv(str(results_path) + \"/test_results.csv\", index_col = \"Date\")\n",
    "test_res.Action = test_res.Action - 1\n",
    "test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BIIO9e2iy3Jo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         Agent Ep NAV  Market Ep NAV  Agent CumSum NAV  Market CumSum NAV  \\\nEpisode                                                                     \n1           -0.261913       0.200659         -0.261913           0.200659   \n2           -0.420464       0.185272         -0.682377           0.385931   \n3           -0.323534      -0.037460         -1.005911           0.348471   \n4           -0.148960       0.024435         -1.154872           0.372906   \n5           -0.304624      -0.365693         -1.459496           0.007214   \n6           -0.139675       0.142511         -1.599171           0.149725   \n7           -0.219994      -0.405698         -1.819165          -0.255973   \n8           -0.413465       0.098297         -2.232630          -0.157676   \n9           -0.099830       0.097443         -2.332460          -0.060234   \n10          -0.407542       0.187833         -2.740002           0.127600   \n11          -0.376525       0.102686         -3.116528           0.230286   \n12          -0.414155       0.074617         -3.530683           0.304902   \n13          -0.337104       0.115549         -3.867787           0.420451   \n14          -0.177953       0.216513         -4.045740           0.636964   \n15          -0.137463      -0.099378         -4.183203           0.537586   \n16           0.043780      -0.388322         -4.139423           0.149264   \n17          -0.275504       0.257876         -4.414928           0.407140   \n18          -0.204296       0.222776         -4.619224           0.629916   \n19          -0.340807       0.042255         -4.960031           0.672171   \n20          -0.033670       0.263664         -4.993700           0.935835   \n\n         Difference  Strategy Wins (%)  \nEpisode                                 \n1         -0.462572                NaN  \n2         -0.605736                NaN  \n3         -0.286074                NaN  \n4         -0.173395                NaN  \n5          0.061068                NaN  \n6         -0.282186                NaN  \n7          0.185704                NaN  \n8         -0.511762                NaN  \n9         -0.197273                NaN  \n10        -0.595376                NaN  \n11        -0.479211                NaN  \n12        -0.488772                NaN  \n13        -0.452652                NaN  \n14        -0.394466                NaN  \n15        -0.038085                NaN  \n16         0.432101                NaN  \n17        -0.533380                NaN  \n18        -0.427072                NaN  \n19        -0.383062                NaN  \n20        -0.297334                NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Agent Ep NAV</th>\n      <th>Market Ep NAV</th>\n      <th>Agent CumSum NAV</th>\n      <th>Market CumSum NAV</th>\n      <th>Difference</th>\n      <th>Strategy Wins (%)</th>\n    </tr>\n    <tr>\n      <th>Episode</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>-0.261913</td>\n      <td>0.200659</td>\n      <td>-0.261913</td>\n      <td>0.200659</td>\n      <td>-0.462572</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.420464</td>\n      <td>0.185272</td>\n      <td>-0.682377</td>\n      <td>0.385931</td>\n      <td>-0.605736</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.323534</td>\n      <td>-0.037460</td>\n      <td>-1.005911</td>\n      <td>0.348471</td>\n      <td>-0.286074</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.148960</td>\n      <td>0.024435</td>\n      <td>-1.154872</td>\n      <td>0.372906</td>\n      <td>-0.173395</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-0.304624</td>\n      <td>-0.365693</td>\n      <td>-1.459496</td>\n      <td>0.007214</td>\n      <td>0.061068</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-0.139675</td>\n      <td>0.142511</td>\n      <td>-1.599171</td>\n      <td>0.149725</td>\n      <td>-0.282186</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-0.219994</td>\n      <td>-0.405698</td>\n      <td>-1.819165</td>\n      <td>-0.255973</td>\n      <td>0.185704</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-0.413465</td>\n      <td>0.098297</td>\n      <td>-2.232630</td>\n      <td>-0.157676</td>\n      <td>-0.511762</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-0.099830</td>\n      <td>0.097443</td>\n      <td>-2.332460</td>\n      <td>-0.060234</td>\n      <td>-0.197273</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-0.407542</td>\n      <td>0.187833</td>\n      <td>-2.740002</td>\n      <td>0.127600</td>\n      <td>-0.595376</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-0.376525</td>\n      <td>0.102686</td>\n      <td>-3.116528</td>\n      <td>0.230286</td>\n      <td>-0.479211</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>-0.414155</td>\n      <td>0.074617</td>\n      <td>-3.530683</td>\n      <td>0.304902</td>\n      <td>-0.488772</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>-0.337104</td>\n      <td>0.115549</td>\n      <td>-3.867787</td>\n      <td>0.420451</td>\n      <td>-0.452652</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-0.177953</td>\n      <td>0.216513</td>\n      <td>-4.045740</td>\n      <td>0.636964</td>\n      <td>-0.394466</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-0.137463</td>\n      <td>-0.099378</td>\n      <td>-4.183203</td>\n      <td>0.537586</td>\n      <td>-0.038085</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.043780</td>\n      <td>-0.388322</td>\n      <td>-4.139423</td>\n      <td>0.149264</td>\n      <td>0.432101</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-0.275504</td>\n      <td>0.257876</td>\n      <td>-4.414928</td>\n      <td>0.407140</td>\n      <td>-0.533380</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>-0.204296</td>\n      <td>0.222776</td>\n      <td>-4.619224</td>\n      <td>0.629916</td>\n      <td>-0.427072</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-0.340807</td>\n      <td>0.042255</td>\n      <td>-4.960031</td>\n      <td>0.672171</td>\n      <td>-0.383062</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>-0.033670</td>\n      <td>0.263664</td>\n      <td>-4.993700</td>\n      <td>0.935835</td>\n      <td>-0.297334</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_res = pd.read_csv(str(results_path) + \"/training_results.csv\", index_col = \"Episode\")\n",
    "train_res\n",
    "# Column \"Strategy Wins (%)\" is NaN because the episode length is 20 (minimum should be 50/100 to get results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [
    "RKFgy8b6XfiU",
    "4soQqr898RiW",
    "RP3fITvTXfiY",
    "1SpuP24pXfii",
    "WsjUsKBA-0u1",
    "l-OJMxB6Xfip",
    "fPfjNvPR8BrL",
    "mzQAmulCVcZX"
   ],
   "machine_shape": "hm",
   "name": "Colab_Notebook_COR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230.906px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}